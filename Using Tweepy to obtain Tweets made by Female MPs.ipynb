{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweepy to Extract Female MP Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code takes code from https://gist.github.com/nicolewhite/167828e51d8f2b6fad75 and modifies it to extract the last 1000 tweets by all female MPs in the UK (as of post GE June 2017) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import Cursor\n",
    "import unicodecsv\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter can be a pain to setup. The main problem I had was this URL field. I solved it by typing in www.google.com. Go figure.\n",
    "\n",
    "Get the salient details and fill in below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authentication and connection to Twitter API.\n",
    "\n",
    "# INSERT YOUR OWN\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Insert People Data Here\n",
    "\n",
    "mps = pd.read_excel(\"./tweet_data/Hackathon_WomenMP.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get just the handles\n",
    "\n",
    "mp_twitter_names = [name for name in mps.twitter_username if type(name) != float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote tweets by VictoriaPrentis to CSV.\n",
      "Wrote tweets by morton_wendy to CSV.\n",
      "Wrote tweets by YasminQureshiMP to CSV.\n",
      "Wrote tweets by YvetteCooperMP to CSV.\n"
     ]
    }
   ],
   "source": [
    "# Usernames whose tweets we want to gather.\n",
    "users = mp_twitter_names\n",
    "\n",
    "with open('tweets.csv', 'wb') as file:\n",
    "    writer = unicodecsv.writer(file, delimiter = ',', quotechar = '\"')\n",
    "    # Write header row with info we want.\n",
    "    writer.writerow([\"politician_name\",\n",
    "                    \"politician_username\",\n",
    "                    \"tweet_year\",\n",
    "                    \"tweet_month\",\n",
    "                    \"tweet_day\",\n",
    "                    \"tweet_hour\",\n",
    "                    \"tweet_text\",\n",
    "                    \"tweet_lat\",\n",
    "                    \"tweet_long\",\n",
    "                    \"tweet_source\",\n",
    "                    \"tweet_in_reply_to_screen_name\",\n",
    "                    \"tweet_direct_reply\",\n",
    "                    \"tweet_retweet_status\",\n",
    "                    \"tweet_retweet_count\",\n",
    "                    \"tweet_favorite_count\",\n",
    "                    \"tweet_hashtags\",\n",
    "                    \"tweet_hashtags_count\",\n",
    "                    \"tweet_urls\",\n",
    "                    \"tweet_urls_count\",\n",
    "                    \"tweet_user_mentions\",\n",
    "                    \"tweet_user_mentions_count\",\n",
    "                    \"tweet_media_type\",\n",
    "                    \"tweet_contributors\"])\n",
    "\n",
    "    for user in users:\n",
    "        user_obj = api.get_user(user)\n",
    "\n",
    "        # Gather info specific to the current user.\n",
    "        user_info = [user_obj.name,\n",
    "                     user_obj.screen_name]\n",
    "\n",
    "        # Get 1000 most recent tweets for the current user.\n",
    "        for tweet in Cursor(api.user_timeline, screen_name = user).items(1000):\n",
    "            # Latitude and longitude stored as array of floats within a dictionary.\n",
    "            lat = tweet.coordinates['coordinates'][1] if tweet.coordinates != None else None\n",
    "            long = tweet.coordinates['coordinates'][0] if tweet.coordinates != None else None\n",
    "            # If tweet is not in reply to a screen name, it is not a direct reply.\n",
    "            direct_reply = True if tweet.in_reply_to_screen_name != \"\" else False\n",
    "            # Retweets start with \"RT ...\"\n",
    "            retweet_status = True if tweet.text[0:3] == \"RT \" else False\n",
    "\n",
    "            # Get info specific to the current tweet of the current user.\n",
    "            tweet_info = [tweet.created_at.year,\n",
    "                          tweet.created_at.month,\n",
    "                          tweet.created_at.day,\n",
    "                          tweet.created_at.hour,\n",
    "                          unidecode(tweet.text),\n",
    "                          lat,\n",
    "                          long,\n",
    "                          tweet.source,\n",
    "                          tweet.in_reply_to_screen_name,\n",
    "                          direct_reply,\n",
    "                          retweet_status,\n",
    "                          tweet.retweet_count,\n",
    "                          tweet.favorite_count]\n",
    "\n",
    "            # Below entities are stored as variable-length dictionaries, if present.\n",
    "            hashtags = []\n",
    "            hashtags_data = tweet.entities.get('hashtags', None)\n",
    "            if(hashtags_data != None):\n",
    "                for i in range(len(hashtags_data)):\n",
    "                    hashtags.append(unidecode(hashtags_data[i]['text']))\n",
    "\n",
    "            urls = []\n",
    "            urls_data = tweet.entities.get('urls', None)\n",
    "            if(urls_data != None):\n",
    "                for i in range(len(urls_data)):\n",
    "                    urls.append(unidecode(urls_data[i]['url']))\n",
    "\n",
    "            user_mentions = []\n",
    "            user_mentions_data = tweet.entities.get('user_mentions', None)\n",
    "            if(user_mentions_data != None):\n",
    "                for i in range(len(user_mentions_data)):\n",
    "                    user_mentions.append(unidecode(user_mentions_data[i]['screen_name']))\n",
    "\n",
    "            media = []\n",
    "            media_data = tweet.entities.get('media', None)\n",
    "            if(media_data != None):\n",
    "                for i in range(len(media_data)):\n",
    "                    media.append(unidecode(media_data[i]['type']))\n",
    "\n",
    "            contributors = []\n",
    "            if(tweet.contributors != None):\n",
    "                for contributor in tweet.contributors:\n",
    "                    contributors.append(unidecode(contributor['screen_name']))\n",
    "\n",
    "            more_tweet_info = [', '.join(hashtags),\n",
    "                               len(hashtags),\n",
    "                               ', '.join(urls),\n",
    "                               len(urls),\n",
    "                               ', '.join(user_mentions),\n",
    "                               len(user_mentions),\n",
    "                               ', '.join(media),\n",
    "                               ', '.join(contributors)]\n",
    "\n",
    "            # Write data to CSV.\n",
    "            writer.writerow(user_info + tweet_info + more_tweet_info)\n",
    "\n",
    "        # Show progress.\n",
    "        print(\"Wrote tweets by %s to CSV.\" % user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Fem_MP_Tweets = pd.DataFrame()\n",
    "\n",
    "path = '../notebooks/FemaleMPTweets/'\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename[-4:] == '.csv':\n",
    "        Fem_MP_Tweets = Fem_MP_Tweets.append(pd.read_csv(path + filename, encoding='latin1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fem_MP_Tweets= Fem_MP_Tweets.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Fem_MP_Tweets.to_pickle('FemaleMPTweets.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
