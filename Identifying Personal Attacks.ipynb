{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a classifier for identifying personal attacks with the Wikipedia Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the fantastic [Jigsaw Paper](http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p1391.pdf) and associated data as an inspiration for creating our own classifier of text which contains personal attacks. Credit goes to [this notebook](https://github.com/ewulczyn/wiki-detox/blob/master/src/figshare/Wikipedia%20Talk%20Data%20-%20Getting%20Started.ipynb) for expediting the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below obtains the labelled dataset from Wikipedia. Effectively, volunteers voted on whether a piece of text in the Wikipedia Talk section was abusive or not. Note that this may not work well for tweets due to the fact that when swearing occurs on Wikipedia, it is usually indicitave of abuse, whereas for Twitter this is not the case. However it is far better than standard sentiment analysis or querying a select corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# panda-fy the data\n",
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LogReg in SKLearn doesn't support continuous vars, so use a 'voting' methodology to determine classification\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279             Iraq is not good  ===  ===  USA is bad   \n",
       "2702703      ____ fuck off you little asshole. If you wan...\n",
       "4632658         i have a dick, its bigger than yours! hahaha\n",
       "6545332      == renault ==  you sad little bpy for drivin...\n",
       "6545351      == renault ==  you sad little bo for driving...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.955\n"
     ]
    }
   ],
   "source": [
    "# fit a Log Reg with character ngrams between 1 and 5 (takes a while)\n",
    "\n",
    "# note that the data already has train/test labels\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features = 10000, ngram_range = (1,5), analyzer='word', norm='l2')),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False], dtype=bool)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so it's not saying this is rude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True], dtype=bool)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nasty comment with obfuscations\n",
    "clf.predict(['You are a f** cu*t!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORKS ON OBFUSCATED CHARACTERS !!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below enhances our logit model by doing a grid search over the hyperparameters with a 4 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the comments to tfidf\n",
    "\n",
    "tfidf_v =  TfidfVectorizer(max_features = 10000, ngram_range = (1,5), analyzer='word', norm='l2')\n",
    "out_spars = tfidf_v.fit_transform(comments['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "params = {'C': [1, 10, 100, 1000], 'penalty': ['l1','l2']}\n",
    "\n",
    "gscv = GridSearchCV(clf,cv=4,param_grid=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.fit(out_spars,comments['attack'])\n",
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  5.61631161,   5.51555151,  10.01550138,  10.03600347,\n",
       "         50.25377482,  14.96274614,  86.61616087,  55.14251369]),\n",
       " 'mean_score_time': array([ 0.0225023 ,  0.02600265,  0.01900196,  0.02100217,  0.02125216,\n",
       "         0.02050203,  0.02050203,  0.02025205]),\n",
       " 'mean_test_score': array([ 0.94428813,  0.94051647,  0.93816026,  0.9417593 ,  0.92476524,\n",
       "         0.93384485,  0.92048436,  0.92488607]),\n",
       " 'mean_train_score': array([ 0.94892   ,  0.94751318,  0.96419653,  0.9597977 ,  0.97028989,\n",
       "         0.96686633,  0.9710609 ,  0.97002521]),\n",
       " 'param_C': masked_array(data = [1 1 10 10 100 100 1000 1000],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_penalty': masked_array(data = ['l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2'],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 1, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 10, 'penalty': 'l1'},\n",
       "  {'C': 10, 'penalty': 'l2'},\n",
       "  {'C': 100, 'penalty': 'l1'},\n",
       "  {'C': 100, 'penalty': 'l2'},\n",
       "  {'C': 1000, 'penalty': 'l1'},\n",
       "  {'C': 1000, 'penalty': 'l2'}),\n",
       " 'rank_test_score': array([1, 3, 4, 2, 7, 5, 8, 6]),\n",
       " 'split0_test_score': array([ 0.94466117,  0.94089826,  0.93710084,  0.94086374,  0.92208375,\n",
       "         0.93268202,  0.91731971,  0.92222184]),\n",
       " 'split0_train_score': array([ 0.94870939,  0.94763916,  0.96465931,  0.95990656,  0.97098864,\n",
       "         0.96745572,  0.97170213,  0.97068944]),\n",
       " 'split1_test_score': array([ 0.94459212,  0.94127801,  0.9378258 ,  0.94258984,  0.92422412,\n",
       "         0.93323437,  0.91956364,  0.92456934]),\n",
       " 'split1_train_score': array([ 0.9490201 ,  0.9475586 ,  0.96414145,  0.95991806,  0.97045928,\n",
       "         0.967122  ,  0.97126483,  0.97018309]),\n",
       " 'split2_test_score': array([ 0.94500259,  0.94130848,  0.93947868,  0.94272398,  0.92680822,\n",
       "         0.93523218,  0.9233903 ,  0.92663559]),\n",
       " 'split2_train_score': array([ 0.94858399,  0.94689237,  0.96358991,  0.95928607,  0.96944729,\n",
       "         0.96596048,  0.9702183 ,  0.96918261]),\n",
       " 'split3_test_score': array([ 0.9428966 ,  0.93858105,  0.9382358 ,  0.94085966,  0.92594511,\n",
       "         0.93423097,  0.92166408,  0.92611773]),\n",
       " 'split3_train_score': array([ 0.94936651,  0.94796258,  0.96439545,  0.96008009,  0.97026433,\n",
       "         0.96692712,  0.97105836,  0.97004569]),\n",
       " 'std_fit_time': array([  2.4009852 ,   0.8962689 ,   0.13418209,   1.0345699 ,\n",
       "         22.62953461,   0.16211551,   7.46983854,  17.46203433]),\n",
       " 'std_score_time': array([ 0.00610388,  0.00616493,  0.00100017,  0.00308253,  0.00334502,\n",
       "         0.00206185,  0.00111814,  0.00082919]),\n",
       " 'std_test_score': array([ 0.00081828,  0.00112902,  0.00086287,  0.00089886,  0.00180618,\n",
       "         0.0009745 ,  0.00227481,  0.00171583]),\n",
       " 'std_train_score': array([ 0.00030276,  0.000389  ,  0.00039521,  0.00030325,  0.00055398,\n",
       "         0.0005561 ,  0.00053915,  0.00054234])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Use L1 regularisation with C = 1."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
